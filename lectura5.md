# Lectura 5

#### Texto: Jahrer, M., Töscher, A. and Legenstein, R. (2010). Combining predictions for accurate recommender systems. In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 693-702. ACM.

El texto aborda la estrategia de “ensemble learning” para sistemas recomendadores utilizando el dataset del Netflix Prize. Esta estrategia consiste en combinar apropiadamente las recomendaciones de varios algoritmos diferentes. Esta combinación (o “blending”) puede darse de varias maneras. La forma más básica sería simplemente calcular un promedio de las recomendaciones de varios algoritmos, mientras que otros métodos consisten en recomendar una combinación lineal de las recomendaciones iniciales. Además, dentro del artículo se explica el procedimiento de “residual learning”, que consiste en encadenar sucesivamente los algoritmos recomendadores a través de sus residuos. Es decir, entrenar un modelo recomendador con los datos de entrenamiento, realizar recomendaciones, calcular los residuos y luego entrenar el siguiente algoritmo sobre esos residuos. De esta forma podemos encadenar una serie de algoritmos y generar recomendaciones más precisas.

El objetivo del paper es mostrar las ventajas del “ensemble learning” a través de un ejemplo práctico donde generan recomendaciones a partir de una combinación lineal de las recomendaciones de 18 algoritmos diferentes. De esta manera, logran un RMSE más bajo que todos los algoritmos utilizados individualmente. Para ello, los autores realizan una revisión de cada uno de los algoritmos en términos generales. Si bien la mayoría de los algoritmos los revisamos en clases, el “Asymmetric factor model” (Paterek, 2007) llamó particularmente mi atención, debido a que podría resolver un problema que tengo en mi proyecto de tesis. Este se plantea como un modelo de factorización matricial típico, es decir, dada una matriz de ratings <a href="https://www.codecogs.com/eqnedit.php?latex=R_{ui}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?R_{ui}" title="R_{ui}" /></a> podemos descomponerla en <a href="https://www.codecogs.com/eqnedit.php?latex=P_u_k&space;\cdot&space;Q_k_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P_u_k&space;\cdot&space;Q_k_i" title="P_u_k \cdot Q_k_i" /></a> donde <a href="https://www.codecogs.com/eqnedit.php?latex=P_u_k" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P_u_k" title="P_u_k" /></a> es la matriz de <a href="https://www.codecogs.com/eqnedit.php?latex=k" target="_blank"><img src="https://latex.codecogs.com/gif.latex?k" title="k" /></a> factores latentes que caracterizan a cada usuario <a href="https://www.codecogs.com/eqnedit.php?latex=u" target="_blank"><img src="https://latex.codecogs.com/gif.latex?u" title="u" /></a> y <a href="https://www.codecogs.com/eqnedit.php?latex=Q_k_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Q_k_i" title="Q_k_i" /></a> es la matriz de <a href="https://www.codecogs.com/eqnedit.php?latex=k" target="_blank"><img src="https://latex.codecogs.com/gif.latex?k" title="k" /></a> factores latentes que caracterizan a los ítems <a href="https://www.codecogs.com/eqnedit.php?latex=i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?i" title="i" /></a>. En este tipo de modelos, cuando la cantidad de usuarios es demasiado grande, la matriz <a href="https://www.codecogs.com/eqnedit.php?latex=P_u_k" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P_u_k" title="P_u_k" /></a> consume demasiada memoria y es lento optimizar sus parámetros. Sin embargo, un modelo AFM solo estima <a href="https://www.codecogs.com/eqnedit.php?latex=Q_k_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Q_k_i" title="Q_k_i" /></a>, mientras que los factores latentes de un usuario u los estima como el promedio de los factores latentes de los ítems consumidos por el usuario. Esto tiene dos ventajas: ahorra memoria y facilita la incorporación de nuevos usuarios al modelo sin la necesidad de reentrenarlo nuevamente.

En el caso de mi tesis, debo realizar un topic model a través de una factorización de matrices en este mismo sentido, sin embargo, se me estaba complicando incorporar todos los documentos necesarios (100 millones de tweets) para esto, debido al gran tamaño de la matriz resultante. Es a partir de esto que pensé que quizás un modelo AFM podría resolver mi problema de memoria, debido a que solo necesitaría alojar en memoria la matriz <a href="https://www.codecogs.com/eqnedit.php?latex=Q_k_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Q_k_i" title="Q_k_i" /></a>, mientras que <a href="https://www.codecogs.com/eqnedit.php?latex=P_u_k" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P_u_k" title="P_u_k" /></a> se calcula a partir de <a href="https://www.codecogs.com/eqnedit.php?latex=Q_k_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Q_k_i" title="Q_k_i" /></a>. De todas formas, tengo que averiguar bien cómo podría implementar esto para el caso de matrices documento-palabra.

Finalmente, volviendo al texto, me gustó la creatividad con la que combinaban los modelos (sobre todo el entrenamiento residual), sin embargo, me hubiese gustado ver un gráfico que compare el tiempo y la memoria invertida en el “ensemble learning”, debido a que sospecho que para este caso no veo muy ventajoso lograr un RMSE de 0.87 si solamente con un modelo SVD lograban un RMSE de 0.88.


* Paterek. A., 2007 - Improving regularized singular value decomposition for collaborative filtering. ***Proceedings of KDD Cup and Workshop***.
